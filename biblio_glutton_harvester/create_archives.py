'''
Modest script to create archives of different type given a large hierarchy of full text resources harvested
by biblio_glutton_harvester and 
following the description for example 
in https://github.com/scilons/documentation/blob/main/references/resource_file_organisation.md

For resources under a path associated to multiple UUID with resource such as PDF, TEI XML, JATS XML, JSON metadata, images assets, thumbnails, etc.
different archives by file type will be created just with PDF, TEI XML, JSON, etc. more precisely:
- one archive with PDF
- one archive with metadata in JSON
- one archive with TEI files
- one archive with NLM/JATS files
- one archive for sources each in one zip, typically LaTeX sources
- one archive with possible generated thumbnail images 
- one archive with extracted embedded images and vector graphics (img_assets) 
- one archive for possible extracted software mentions 
- one archive for possible extracted dataset mentions 

In each of these archives, the same directory structure is preserved.
'''

import os
import sys
import time
import tarfile
import argparse
import yaml
from pathlib import Path
from zipfile import ZIP_DEFLATED, ZipFile

file_types = [ "pdf", "tei", "jats", "latex_sources", "img_assets", "metadata", "thumb", "dataset", "software"] 

def create_archives(input_raw, output=None, archive_type=None):
    nb_archives_files = 0
    all_lists = {}
    for file_type in file_types:
        all_lists[file_type] = []

    for root, directories, filenames in os.walk(input_raw):
        for filename in filenames: 
            subroot = root.replace(input_raw, "")

            # there might be some incomplete files at the root path that we want to skip 
            if len(subroot.split("/")) <= 2:
                continue

            if root.endswith("latex") or filename.endswith(".zip"):
                # LaTeX sources (arXiv)
                all_lists["latex_sources"].append(os.path.join(subroot,filename))
                
            elif root.endswith("_assets"):
                # embedded bitmap and SVG from Grobid
                all_lists["img_assets"].append(os.path.join(subroot,filename))

            elif filename.endswith(".pdf") or filename.endswith(".PDF"):
                all_lists["pdf"].append(os.path.join(subroot,filename))

            elif filename.endswith(".tei.xml"):
                # structured fulltext produced either by Grobid or Pub2TEI or latexml
                all_lists["tei"].append(os.path.join(subroot,filename))

            elif filename.endswith(".jats.xml") or filename.endswith(".nxml"):
                # JATS/NLM source (but also normally converted to TEI XML by Pub2TEI)
                all_lists["jats"].append(os.path.join(subroot,filename))

            elif filename.endswith(".software.json"):
                # extracted software mentions of the document if present
                all_lists["software"].append(os.path.join(subroot,filename))

            elif filename.endswith(".dataset.json"):
                # extracted software mentions of the document if present
                all_lists["dataset"].append(os.path.join(subroot,filename))

            elif filename.endswith(".json"):
                # metadata of the file (typically enriched crossref entry)
                all_lists["metadata"].append(os.path.join(subroot,filename))

            elif filename.endswith(".thumb-small.png") or filename.endswith(".thumb-medium.png") or filename.endswith(".thumb-large.png"):
                # thumbnail of the first page of the PDF generated by the harvester (optional)
                all_lists["thumb"].append(os.path.join(subroot,filename))

    for file_type in file_types:
        if len(all_lists[file_type])>0:
            if archive_type != None and archive_type == "tar":
                make_tarfile(os.path.join(output,file_type+".tar.gz"), all_lists[file_type], input_raw)
            else:
                make_zip(os.path.join(output,file_type+".zip"), all_lists[file_type], input_raw)
            nb_archives_files += 1

    return nb_archives_files


def make_zip(archive_file_path, source_file_paths, base_path):
    output_filename = archive_file_path
    with ZipFile(output_filename, 'w', ZIP_DEFLATED) as zip_object:
        for file_path in source_file_paths:
            zip_object.write(os.path.join(base_path,file_path), arcname=file_path)

def make_tarfile(archive_file_path, source_file_paths, base_path):
    output_filename = archive_file_path
    with tarfile.open(output_filename, "w:gz") as tar:
        for file_path in source_file_paths:
            tar.add(os.path.join(base_path,file_path), arcname=file_path)

if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="Create archives of full text resources by file types")
    parser.add_argument("--input", type=str, help="path to the repository of full text resources using UUID distribution and directory structure")
    parser.add_argument("--output", type=str, help="path where to write the archives")

    output_raw = None

    args = parser.parse_args()
    input_raw = args.input
    output_raw = args.output
    
    # check output path
    if output_raw == None:
        print("output path not specified, the current path will be used to write the archives")
    elif not os.path.exists(output_raw):
        print("invalid output path:", output_raw)
        sys.exit(1)
    elif not os.path.isdir(output_raw):
        print("output path is not a pth to a directory:", output_raw)
        sys.exit(1)

    start_time = time.time()
    nb_archive_files = 0

    if input_raw == None:
        print("usage: python3 -m harvesting.create_archives --input [path to the directory of full text resources]")
    elif not os.path.exists(input_raw):
        print("invalid resource path:", input_raw)
    elif os.path.isdir(input_raw):
        try:
            nb_archive_files = create_archives(input_raw, output=output_raw)
        except Exception as e:
            print("Failed to create archives", input_raw, str(e))
            sys.exit(1)
    else:
        print("not a directory path:", input_raw)

    runtime = round(time.time() - start_time, 3)
    print("%s archive created in %s seconds " % (nb_archive_files, runtime))
